{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7458d81e-776f-4daa-b526-e2c44e499cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
      "  Collecting setuptools>=42\n",
      "    Downloading setuptools-70.1.1-py3-none-any.whl (883 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.3/883.3 KB 9.8 MB/s eta 0:00:00\n",
      "  Collecting scikit-build>=0.13\n",
      "    Downloading scikit_build-0.18.0-py3-none-any.whl (85 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 KB 5.2 MB/s eta 0:00:00\n",
      "  Collecting cmake>=3.18\n",
      "    Downloading cmake-3.29.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 9.9 MB/s eta 0:00:00\n",
      "  Collecting ninja\n",
      "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "  Collecting wheel>=0.32.0\n",
      "    Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "  Collecting packaging\n",
      "    Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 KB 3.9 MB/s eta 0:00:00\n",
      "  Collecting distro\n",
      "    Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "  Collecting tomli\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n",
      "  Successfully installed cmake-3.29.6 distro-1.9.0 ninja-1.11.1.1 packaging-24.1 scikit-build-0.18.0 setuptools-70.1.1 tomli-2.0.1 wheel-0.43.0\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-uhxygbxp/llama_cpp_python.egg-info\n",
      "  writing manifest file '/tmp/pip-modern-metadata-uhxygbxp/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-uhxygbxp/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numpy==1.23.4\n",
      "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 KB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (4.0s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "    Command:\n",
      "      /tmp/pip-build-env-kht6v6r_/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-kht6v6r_/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-kht6v6r_/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython_NumPy_INCLUDE_DIRS:PATH=/usr/lib/python3/dist-packages/numpy/core/include -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_NumPy_INCLUDE_DIRS:PATH=/usr/lib/python3/dist-packages/numpy/core/include -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-kht6v6r_/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "  \u001b[0m\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Unable to find cudart library.\n",
      "  -- Could NOT find CUDAToolkit (missing: CUDA_CUDART) (found version \"11.8.89\")\n",
      "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:291 (message):\n",
      "    cuBLAS not found\n",
      "\n",
      "  \u001b[0m\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  -- Configuring done (4.4s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "  [1/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
      "  [2/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
      "  [3/8] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
      "  [4/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
      "  [5/8] Linking C static library vendor/llama.cpp/libggml_static.a\n",
      "  [6/8] Linking C shared library vendor/llama.cpp/libggml_shared.so\n",
      "  [7/8] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
      "  [7/8] Install the project...\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
      "  -- Installing: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
      "  -- Installing: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
      "\n",
      "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
      "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
      "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
      "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
      "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
      "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
      "  copying /tmp/pip-install-bhxfj4ab/llama-cpp-python_9c27eacac45f457fa7603196a8d69fa9/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  running build_ext\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  running install_data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.egg-info\n",
      "  running install_scripts\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=729822 sha256=3f3a94b2afc6e57e1c1bfa090dcfda1ce56f6ff2a853dff69f3aa6ca92a9a6e6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eku6_hoi/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/typing_extensions-4.11.0.dist-info/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Removing file or directory /home/edg3/.local/bin/f2py\n",
      "      Removing file or directory /home/edg3/.local/bin/f2py3\n",
      "      Removing file or directory /home/edg3/.local/bin/f2py3.10\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/numpy-1.24.4.dist-info/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/numpy.libs/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/numpy/\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  changing mode of /home/edg3/.local/bin/f2py to 755\n",
      "  changing mode of /home/edg3/.local/bin/f2py3 to 755\n",
      "  changing mode of /home/edg3/.local/bin/f2py3.10 to 755\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/diskcache-5.6.3.dist-info/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.60\n",
      "    Uninstalling llama_cpp_python-0.2.60:\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/bin/__pycache__/convert.cpython-310.pyc\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/bin/convert-lora-to-ggml.py\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/bin/convert.py\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/bin/llava-cli\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/include/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/lib/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/llama_cpp/\n",
      "      Removing file or directory /home/edg3/.local/lib/python3.10/site-packages/llama_cpp_python-0.2.60.dist-info/\n",
      "      Successfully uninstalled llama_cpp_python-0.2.60\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.12.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/edg3/.local/lib/python3.10/site-packages (0.23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/edg3/.local/lib/python3.10/site-packages (from huggingface_hub) (3.13.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/edg3/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/edg3/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/edg3/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/edg3/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python==0.1.78 in /home/edg3/.local/lib/python3.10/site-packages (0.1.78)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/edg3/.local/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/edg3/.local/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/edg3/.local/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (4.12.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.23.4 in /home/edg3/.local/lib/python3.10/site-packages (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python==0.1.78\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1431aaf-41e3-47f1-bf87-8fe94e150589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b183211-860a-40aa-b6e8-2452e72e66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17495960-0789-4c16-bdca-304cd88a00f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d491608b9f8143b59a6b3d215c7b8b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e87674-a2f7-4bb5-9a67-5c7a431028d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/edg3/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c132c5-e82f-4aa4-a287-cffb9bbb1ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4187de5-0b37-4223-bad0-4d162d139ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression in python\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c414790-8ee0-4c29-95b7-96f50a009457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 16866.09 ms\n",
      "llama_print_timings:      sample time =   113.61 ms /   256 runs   (    0.44 ms per token,  2253.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16865.94 ms /    39 tokens (  432.46 ms per token,     2.31 tokens per second)\n",
      "llama_print_timings:        eval time = 139750.87 ms /   255 runs   (  548.04 ms per token,     1.82 tokens per second)\n",
      "llama_print_timings:       total time = 157158.94 ms\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccfb94b-1e5f-477a-a9fe-a93884c15eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-6c824b6d-9733-4520-bc4a-503b9e60f0a6', 'object': 'text_completion', 'created': 1719558949, 'model': '/home/edg3/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': \"SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression in python\\n\\nASSISTANT:\\n\\nTo write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\\n```\\nfrom sklearn.linear_model import LinearRegression\\nimport pandas as pd\\n\\n# Load your dataset into a Pandas DataFrame\\ndf = pd.read_csv('your_data.csv')\\n\\n# Create a linear regression object and fit the data\\nreg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\\n\\n# Print the coefficients\\nprint(reg.coef_)\\n\\n# Predict on new data\\nnew_data = pd.DataFrame({'x1': [1, 2, 3], 'x2': [4, 5, 6]})\\npreds = reg.predict(new_data)\\n```\\nThis code will load your dataset into a Pandas DataFrame, create a linear regression object and fit the data using the `fit()` method. It will then print the coefficients of the linear regression and use those coefficients to make predictions on new data.\\n\\nPlease note that this is just an example, you may need to modify it to suit your specific needs\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 256, 'total_tokens': 295}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed1ad7d-6b79-44b2-907e-54f5b373a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression in python\n",
      "\n",
      "ASSISTANT:\n",
      "\n",
      "To write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\n",
      "```\n",
      "from sklearn.linear_model import LinearRegression\n",
      "import pandas as pd\n",
      "\n",
      "# Load your dataset into a Pandas DataFrame\n",
      "df = pd.read_csv('your_data.csv')\n",
      "\n",
      "# Create a linear regression object and fit the data\n",
      "reg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\n",
      "\n",
      "# Print the coefficients\n",
      "print(reg.coef_)\n",
      "\n",
      "# Predict on new data\n",
      "new_data = pd.DataFrame({'x1': [1, 2, 3], 'x2': [4, 5, 6]})\n",
      "preds = reg.predict(new_data)\n",
      "```\n",
      "This code will load your dataset into a Pandas DataFrame, create a linear regression object and fit the data using the `fit()` method. It will then print the coefficients of the linear regression and use those coefficients to make predictions on new data.\n",
      "\n",
      "Please note that this is just an example, you may need to modify it to suit your specific needs\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f9234-d782-4fbc-8151-673cc80090d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
