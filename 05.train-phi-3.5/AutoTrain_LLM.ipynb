{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target:\n",
    "\n",
    "Training of [microsoft/Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) with (slightly messy) Godot 4.3 documentation.\n",
    "\n",
    "## Desired Outcome\n",
    "\n",
    "More reliable Q&A about Godot 4.3 on ```edg3/godot-4.3-phi-3.5-mini-instruct```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤— AutoTrain LLM\n",
    "\n",
    "In order to use this colab:\n",
    "\n",
    "- upload train.csv to a folder named `data/`\n",
    "- train.csv must contain a `text` column\n",
    "- choose a project name if you wish\n",
    "- change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
    "- add huggingface information (token) if you wish to push trained model to huggingface hub\n",
    "- update hyperparameters if you wish\n",
    "- click `Runtime > Run all` or run each cell individually\n",
    "- they share you can report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get AutoTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U autotrain-advanced --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:08:33\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mInstalling latest xformers\u001b[0m\r\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:08:44\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSuccessfully installed latest xformers\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!autotrain setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "JvMRbVLEJlZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTrain version: 0.8.21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autotrain import __version__\n",
    "print(f'AutoTrain version: {__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Config\n",
    "\n",
    "Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'godot43llama27b' # folder you will create\n",
    "model_name = 'content/' # where your model is (e.g. 'git clone ...' renamed to 'content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push to Hub?\n",
    "\n",
    "Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "\n",
    "If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "\n",
    "Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "\n",
    "You can find your token here: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "A2-_lkBS1WKA"
   },
   "outputs": [],
   "source": [
    "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"hf_XXX\" #@param {type:\"string\"}\n",
    "hf_username = \"abc\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsloth = False             # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "learning_rate = 2e-4        # @param {type:\"number\"}\n",
    "num_epochs = 1              # @param {type:\"number\"}\n",
    "batch_size = 1              # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "block_size = 1024           # @param {type:\"number\"}\n",
    "trainer = \"sft\"             # @param [\"generic\", \"sft\"] {type:\"string\"}\n",
    "warmup_ratio = 0.1          # @param {type:\"number\"}\n",
    "weight_decay = 0.01         # @param {type:\"number\"}\n",
    "gradient_accumulation = 4   # @param {type:\"number\"}\n",
    "mixed_precision = \"fp16\"    # @param [\"fp16\", \"bf16\", \"none\"] {type:\"string\"}\n",
    "peft = True                 # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "quantization = \"int4\"       # @param [\"int4\", \"int8\", \"none\"] {type:\"string\"}\n",
    "lora_r = 16                 #@param {type:\"number\"}\n",
    "lora_alpha = 32             #@param {type:\"number\"}\n",
    "lora_dropout = 0.05         #@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Environment Vars and Conf.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"HF_USERNAME\"] = hf_username\n",
    "\n",
    "conf = f\"\"\"\n",
    "task: llm-{trainer}\n",
    "base_model: {model_name}\n",
    "project_name: {project_name}\n",
    "log: tensorboard\n",
    "backend: local\n",
    "\n",
    "data:\n",
    "  path: data/\n",
    "  train_split: train\n",
    "  valid_split: null\n",
    "  chat_template: null\n",
    "  column_mapping:\n",
    "    text_column: text\n",
    "\n",
    "params:\n",
    "  block_size: {block_size}\n",
    "  lr: {learning_rate}\n",
    "  warmup_ratio: {warmup_ratio}\n",
    "  weight_decay: {weight_decay}\n",
    "  epochs: {num_epochs}\n",
    "  batch_size: {batch_size}\n",
    "  gradient_accumulation: {gradient_accumulation}\n",
    "  mixed_precision: {mixed_precision}\n",
    "  peft: {peft}\n",
    "  quantization: {quantization}\n",
    "  lora_r: {lora_r}\n",
    "  lora_alpha: {lora_alpha}\n",
    "  lora_dropout: {lora_dropout}\n",
    "  unsloth: {unsloth}\n",
    "\n",
    "hub:\n",
    "  username: ${{HF_USERNAME}}\n",
    "  token: ${{HF_TOKEN}}\n",
    "  push_to_hub: {push_to_hub}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf.yaml\", \"w\") as f:\n",
    "  f.write(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q aqlm[gpu,cpu]  --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g3cd_ED_yXXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:31\u001b[0m | \u001b[36mautotrain.cli.autotrain\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mUsing AutoTrain configuration: conf.yaml\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mRunning task: lm_training\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mUsing backend: local\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m{'model': 'content/', 'project_name': 'godot43llama27b', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': None, 'text_column': 'text', 'rejected_text_column': None, 'push_to_hub': False, 'username': 'abc', 'token': '*****', 'unsloth': False, 'distributed_backend': None}\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'godot43llama27b/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:09:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1m{'model': 'content/', 'project_name': 'godot43llama27b', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': None, 'text_column': 'text', 'rejected_text_column': None, 'push_to_hub': False, 'username': 'abc', 'token': '*****', 'unsloth': False, 'distributed_backend': None}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:10\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "Generating train split: 118926 examples [00:00, 188682.92 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m398\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 118926\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m629\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:10:12\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m639\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 152.73s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:15:19\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:15:20\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "Generating train split: 3873 examples [00:11, 334.94 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:15:34\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m230\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "  0%|                                                   | 0/968 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "/home/edg3/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "  3%|â–ˆ                                       | 25/968 [06:21<3:58:44, 15.19s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:21:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 2.7849, 'grad_norm': 0.8068005442619324, 'learning_rate': 4.948453608247423e-05, 'epoch': 0.025819777949909632}\u001b[0m\n",
      "{'loss': 2.7849, 'grad_norm': 0.8068005442619324, 'learning_rate': 4.948453608247423e-05, 'epoch': 0.03}\n",
      "  5%|â–ˆâ–ˆ                                      | 50/968 [12:38<3:51:05, 15.10s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:28:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 2.1508, 'grad_norm': 0.44694361090660095, 'learning_rate': 0.00010103092783505155, 'epoch': 0.051639555899819264}\u001b[0m\n",
      "{'loss': 2.1508, 'grad_norm': 0.44694361090660095, 'learning_rate': 0.00010103092783505155, 'epoch': 0.05}\n",
      "  8%|â–ˆâ–ˆâ–ˆ                                     | 75/968 [18:56<3:45:42, 15.17s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:34:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.7834, 'grad_norm': 0.43991711735725403, 'learning_rate': 0.00015257731958762886, 'epoch': 0.07745933384972889}\u001b[0m\n",
      "{'loss': 1.7834, 'grad_norm': 0.43991711735725403, 'learning_rate': 0.00015257731958762886, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆ                                   | 100/968 [25:13<3:37:31, 15.04s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:40:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.6904, 'grad_norm': 0.6797800064086914, 'learning_rate': 0.000199540757749713, 'epoch': 0.10327911179963853}\u001b[0m\n",
      "{'loss': 1.6904, 'grad_norm': 0.6797800064086914, 'learning_rate': 0.000199540757749713, 'epoch': 0.1}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 125/968 [31:31<3:33:06, 15.17s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:47:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4407, 'grad_norm': 0.4424688518047333, 'learning_rate': 0.00019380022962112517, 'epoch': 0.12909888974954814}\u001b[0m\n",
      "{'loss': 1.4407, 'grad_norm': 0.4424688518047333, 'learning_rate': 0.00019380022962112517, 'epoch': 0.13}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 150/968 [37:49<3:25:45, 15.09s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:53:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.5903, 'grad_norm': 0.47043392062187195, 'learning_rate': 0.00018805970149253734, 'epoch': 0.15491866769945778}\u001b[0m\n",
      "{'loss': 1.5903, 'grad_norm': 0.47043392062187195, 'learning_rate': 0.00018805970149253734, 'epoch': 0.15}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 175/968 [44:08<3:22:50, 15.35s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 14:59:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4739, 'grad_norm': 0.48234713077545166, 'learning_rate': 0.0001823191733639495, 'epoch': 0.18073844564936742}\u001b[0m\n",
      "{'loss': 1.4739, 'grad_norm': 0.48234713077545166, 'learning_rate': 0.0001823191733639495, 'epoch': 0.18}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 200/968 [50:28<3:12:55, 15.07s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:06:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4862, 'grad_norm': 0.5201423764228821, 'learning_rate': 0.00017657864523536168, 'epoch': 0.20655822359927706}\u001b[0m\n",
      "{'loss': 1.4862, 'grad_norm': 0.5201423764228821, 'learning_rate': 0.00017657864523536168, 'epoch': 0.21}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 225/968 [56:46<3:07:56, 15.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:12:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4115, 'grad_norm': 0.47474685311317444, 'learning_rate': 0.00017083811710677383, 'epoch': 0.23237800154918667}\u001b[0m\n",
      "{'loss': 1.4115, 'grad_norm': 0.47474685311317444, 'learning_rate': 0.00017083811710677383, 'epoch': 0.23}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 250/968 [1:03:03<3:00:07, 15.05s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:18:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4702, 'grad_norm': 0.43839696049690247, 'learning_rate': 0.000165097588978186, 'epoch': 0.2581977794990963}\u001b[0m\n",
      "{'loss': 1.4702, 'grad_norm': 0.43839696049690247, 'learning_rate': 0.000165097588978186, 'epoch': 0.26}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 275/968 [1:09:21<2:55:26, 15.19s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:24:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.5464, 'grad_norm': 0.5761700868606567, 'learning_rate': 0.00015935706084959817, 'epoch': 0.2840175574490059}\u001b[0m\n",
      "{'loss': 1.5464, 'grad_norm': 0.5761700868606567, 'learning_rate': 0.00015935706084959817, 'epoch': 0.28}\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 300/968 [1:15:39<2:47:49, 15.07s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:31:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4095, 'grad_norm': 0.47183936834335327, 'learning_rate': 0.00015361653272101032, 'epoch': 0.30983733539891556}\u001b[0m\n",
      "{'loss': 1.4095, 'grad_norm': 0.47183936834335327, 'learning_rate': 0.00015361653272101032, 'epoch': 0.31}\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 325/968 [1:21:57<2:43:17, 15.24s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:37:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4368, 'grad_norm': 0.4742618203163147, 'learning_rate': 0.0001478760045924225, 'epoch': 0.3356571133488252}\u001b[0m\n",
      "{'loss': 1.4368, 'grad_norm': 0.4742618203163147, 'learning_rate': 0.0001478760045924225, 'epoch': 0.34}\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 350/968 [1:28:15<2:34:56, 15.04s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:43:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4141, 'grad_norm': 0.5102396607398987, 'learning_rate': 0.00014213547646383466, 'epoch': 0.36147689129873484}\u001b[0m\n",
      "{'loss': 1.4141, 'grad_norm': 0.5102396607398987, 'learning_rate': 0.00014213547646383466, 'epoch': 0.36}\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 375/968 [1:34:35<2:33:37, 15.54s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:50:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3256, 'grad_norm': 0.5140380859375, 'learning_rate': 0.00013639494833524683, 'epoch': 0.3872966692486445}\u001b[0m\n",
      "{'loss': 1.3256, 'grad_norm': 0.5140380859375, 'learning_rate': 0.00013639494833524683, 'epoch': 0.39}\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 400/968 [1:41:02<2:25:09, 15.33s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 15:56:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2819, 'grad_norm': 0.4948917031288147, 'learning_rate': 0.000130654420206659, 'epoch': 0.4131164471985541}\u001b[0m\n",
      "{'loss': 1.2819, 'grad_norm': 0.4948917031288147, 'learning_rate': 0.000130654420206659, 'epoch': 0.41}\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 425/968 [1:47:21<2:17:04, 15.15s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:02:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2334, 'grad_norm': 0.49356165528297424, 'learning_rate': 0.00012491389207807118, 'epoch': 0.4389362251484637}\u001b[0m\n",
      "{'loss': 1.2334, 'grad_norm': 0.49356165528297424, 'learning_rate': 0.00012491389207807118, 'epoch': 0.44}\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 450/968 [1:53:39<2:09:41, 15.02s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:09:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3526, 'grad_norm': 0.5089042782783508, 'learning_rate': 0.00011917336394948335, 'epoch': 0.46475600309837334}\u001b[0m\n",
      "{'loss': 1.3526, 'grad_norm': 0.5089042782783508, 'learning_rate': 0.00011917336394948335, 'epoch': 0.46}\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 475/968 [2:00:11<2:09:10, 15.72s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:15:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3527, 'grad_norm': 0.5784817934036255, 'learning_rate': 0.00011343283582089552, 'epoch': 0.490575781048283}\u001b[0m\n",
      "{'loss': 1.3527, 'grad_norm': 0.5784817934036255, 'learning_rate': 0.00011343283582089552, 'epoch': 0.49}\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 500/968 [2:06:45<2:02:04, 15.65s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:22:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3842, 'grad_norm': 0.49751871824264526, 'learning_rate': 0.0001076923076923077, 'epoch': 0.5163955589981926}\u001b[0m\n",
      "{'loss': 1.3842, 'grad_norm': 0.49751871824264526, 'learning_rate': 0.0001076923076923077, 'epoch': 0.52}\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 525/968 [2:13:15<1:51:46, 15.14s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:28:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3313, 'grad_norm': 0.5155372023582458, 'learning_rate': 0.00010195177956371987, 'epoch': 0.5422153369481022}\u001b[0m\n",
      "{'loss': 1.3313, 'grad_norm': 0.5155372023582458, 'learning_rate': 0.00010195177956371987, 'epoch': 0.54}\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 550/968 [2:19:33<1:45:39, 15.17s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:35:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3732, 'grad_norm': 0.5241450071334839, 'learning_rate': 9.621125143513204e-05, 'epoch': 0.5680351148980118}\u001b[0m\n",
      "{'loss': 1.3732, 'grad_norm': 0.5241450071334839, 'learning_rate': 9.621125143513204e-05, 'epoch': 0.57}\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 575/968 [2:25:50<1:38:39, 15.06s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:41:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2062, 'grad_norm': 0.5038409233093262, 'learning_rate': 9.047072330654421e-05, 'epoch': 0.5938548928479215}\u001b[0m\n",
      "{'loss': 1.2062, 'grad_norm': 0.5038409233093262, 'learning_rate': 9.047072330654421e-05, 'epoch': 0.59}\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 600/968 [2:32:07<1:33:01, 15.17s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:47:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3515, 'grad_norm': 0.5237393379211426, 'learning_rate': 8.473019517795637e-05, 'epoch': 0.6196746707978311}\u001b[0m\n",
      "{'loss': 1.3515, 'grad_norm': 0.5237393379211426, 'learning_rate': 8.473019517795637e-05, 'epoch': 0.62}\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 625/968 [2:38:24<1:26:01, 15.05s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 16:53:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3099, 'grad_norm': 0.5490126609802246, 'learning_rate': 7.898966704936854e-05, 'epoch': 0.6454944487477408}\u001b[0m\n",
      "{'loss': 1.3099, 'grad_norm': 0.5490126609802246, 'learning_rate': 7.898966704936854e-05, 'epoch': 0.65}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 650/968 [2:44:41<1:19:24, 14.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:00:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4138, 'grad_norm': 0.5774059891700745, 'learning_rate': 7.324913892078071e-05, 'epoch': 0.6713142266976504}\u001b[0m\n",
      "{'loss': 1.4138, 'grad_norm': 0.5774059891700745, 'learning_rate': 7.324913892078071e-05, 'epoch': 0.67}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 675/968 [2:50:59<1:13:58, 15.15s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:06:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.1895, 'grad_norm': 0.5040886998176575, 'learning_rate': 6.750861079219288e-05, 'epoch': 0.69713400464756}\u001b[0m\n",
      "{'loss': 1.1895, 'grad_norm': 0.5040886998176575, 'learning_rate': 6.750861079219288e-05, 'epoch': 0.7}\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 700/968 [2:57:15<1:07:00, 15.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:12:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.4607, 'grad_norm': 0.6330024003982544, 'learning_rate': 6.176808266360506e-05, 'epoch': 0.7229537825974697}\u001b[0m\n",
      "{'loss': 1.4607, 'grad_norm': 0.6330024003982544, 'learning_rate': 6.176808266360506e-05, 'epoch': 0.72}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 725/968 [3:03:33<1:01:11, 15.11s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:19:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3856, 'grad_norm': 0.5823684334754944, 'learning_rate': 5.602755453501722e-05, 'epoch': 0.7487735605473793}\u001b[0m\n",
      "{'loss': 1.3856, 'grad_norm': 0.5823684334754944, 'learning_rate': 5.602755453501722e-05, 'epoch': 0.75}\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 750/968 [3:10:01<57:36, 15.86s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:25:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3075, 'grad_norm': 0.5351530313491821, 'learning_rate': 5.028702640642939e-05, 'epoch': 0.774593338497289}\u001b[0m\n",
      "{'loss': 1.3075, 'grad_norm': 0.5351530313491821, 'learning_rate': 5.028702640642939e-05, 'epoch': 0.77}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 775/968 [3:16:35<50:52, 15.82s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:32:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2113, 'grad_norm': 0.5041967630386353, 'learning_rate': 4.4546498277841564e-05, 'epoch': 0.8004131164471986}\u001b[0m\n",
      "{'loss': 1.2113, 'grad_norm': 0.5041967630386353, 'learning_rate': 4.4546498277841564e-05, 'epoch': 0.8}\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 800/968 [3:23:09<44:13, 15.79s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:38:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2885, 'grad_norm': 0.5345399975776672, 'learning_rate': 3.8805970149253736e-05, 'epoch': 0.8262328943971082}\u001b[0m\n",
      "{'loss': 1.2885, 'grad_norm': 0.5345399975776672, 'learning_rate': 3.8805970149253736e-05, 'epoch': 0.83}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 825/968 [3:29:43<37:44, 15.83s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:45:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3034, 'grad_norm': 0.5494066476821899, 'learning_rate': 3.30654420206659e-05, 'epoch': 0.8520526723470179}\u001b[0m\n",
      "{'loss': 1.3034, 'grad_norm': 0.5494066476821899, 'learning_rate': 3.30654420206659e-05, 'epoch': 0.85}\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 850/968 [3:36:16<31:03, 15.79s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:51:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2647, 'grad_norm': 0.5681136846542358, 'learning_rate': 2.7324913892078073e-05, 'epoch': 0.8778724502969274}\u001b[0m\n",
      "{'loss': 1.2647, 'grad_norm': 0.5681136846542358, 'learning_rate': 2.7324913892078073e-05, 'epoch': 0.88}\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 875/968 [3:42:49<24:28, 15.79s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 17:58:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.3211, 'grad_norm': 0.5889748930931091, 'learning_rate': 2.1584385763490242e-05, 'epoch': 0.903692228246837}\u001b[0m\n",
      "{'loss': 1.3211, 'grad_norm': 0.5889748930931091, 'learning_rate': 2.1584385763490242e-05, 'epoch': 0.9}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 900/968 [3:49:22<17:53, 15.78s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:04:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2365, 'grad_norm': 0.5773370862007141, 'learning_rate': 1.584385763490241e-05, 'epoch': 0.9295120061967467}\u001b[0m\n",
      "{'loss': 1.2365, 'grad_norm': 0.5773370862007141, 'learning_rate': 1.584385763490241e-05, 'epoch': 0.93}\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 925/968 [3:55:54<11:11, 15.61s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:11:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2396, 'grad_norm': 0.5994133949279785, 'learning_rate': 1.010332950631458e-05, 'epoch': 0.9553317841466563}\u001b[0m\n",
      "{'loss': 1.2396, 'grad_norm': 0.5994133949279785, 'learning_rate': 1.010332950631458e-05, 'epoch': 0.96}\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 950/968 [4:02:26<04:41, 15.64s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:18:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'loss': 1.2133, 'grad_norm': 0.5288025140762329, 'learning_rate': 4.362801377726751e-06, 'epoch': 0.981151562096566}\u001b[0m\n",
      "{'loss': 1.2133, 'grad_norm': 0.5288025140762329, 'learning_rate': 4.362801377726751e-06, 'epoch': 0.98}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 968/968 [4:07:09<00:00, 15.71s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:22:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m{'train_runtime': 14829.1141, 'train_samples_per_second': 0.261, 'train_steps_per_second': 0.065, 'train_loss': 1.4289995303823928, 'epoch': 0.9997418022205009}\u001b[0m\n",
      "{'train_runtime': 14829.1141, 'train_samples_per_second': 0.261, 'train_steps_per_second': 0.065, 'train_loss': 1.4289995303823928, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 968/968 [4:07:09<00:00, 15.32s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:22:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mpost_training_steps\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-10-09 18:22:56\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mJob ID: 1070\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain --config conf.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 06:54:55.384390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 06:54:56.023479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 06:54:56.167511: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 06:54:57.077188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 06:55:04.281703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb7226d2ea64decb2882e0340554be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Phi3ForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32011, 3072]) from checkpoint, the shape in current model is torch.Size([32064, 3072]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32011, 3072]) from checkpoint, the shape in current model is torch.Size([32064, 3072]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgodot43llama27b/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgodot43llama27b/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mQuery\u001b[39m(message):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:4092\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4089\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4092\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4093\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4094\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4096\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   4100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/integrations/peft.py:214\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     processed_adapter_state_dict[new_key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_adapter_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incompatible_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# check only for unexpected keys\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(incompatible_keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(incompatible_keys\u001b[38;5;241m.\u001b[39munexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:460\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    458\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(peft_model_state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, assign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    463\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    464\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Phi3ForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32011, 3072]) from checkpoint, the shape in current model is torch.Size([32064, 3072]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32011, 3072]) from checkpoint, the shape in current model is torch.Size([32064, 3072])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"godot43llama27b/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"godot43llama27b/\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def Query(message):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1: Whatâ€™s new in Godot 4.3 compared to previous versions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('Whatâ€™s new in Godot 4.3 compared to previous versions?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2: Can you explain the changes in the rendering pipeline introduced in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('Can you explain the changes in the rendering pipeline introduced in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3: How do you implement a custom shader in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('How do you implement a custom shader in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4: What are the key differences between GDScript and C# in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('What are the key differences between GDScript and C# in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5: How would you optimize a game for performance in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('How would you optimize a game for performance in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6: Can you describe the process of creating a custom node in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('Can you describe the process of creating a custom node in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7: What are the new features in the animation system of Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('What are the new features in the animation system of Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8: How do you handle input events in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('How do you handle input events in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9: What are the best practices for using the new navigation system in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('What are the best practices for using the new navigation system in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.10: Can you walk me through setting up a multiplayer game in Godot 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('Can you walk me through setting up a multiplayer game in Godot 4.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.11: How can I use C# to create a procedurally generated 3D chunk in a node in Godot 4.3 on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('How can I use C# to create a procedurally generated 3D chunk in a node in Godot 4.3 on its own?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.12: How can I make nodes used as chunks, similar to minecraft, to store data in Godot 4.3 so that I can create a synchronous multiplayer world that can be hosted by 1 player, and have a second player connect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('How can I make nodes used as chunks, similar to minecraft, to store data in Godot 4.3 so that I can create a synchronous multiplayer world that can be hosted by 1 player, and have a second player connect?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.13: In Godot 4.3, using C#, can you create 2D snake from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('In Godot 4.3, using C#, can you create 2D snake from scratch?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review (Personal):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import create_repo\n",
    "import os\n",
    "\n",
    "# Define your model path and repository name\n",
    "model_path = \"godot43llama27b/\"\n",
    "repo_name = \"edg3/godot-4.3-phi-3.5-mini-instruct\"\n",
    "my_token = '<your-token-here>' \n",
    "\n",
    "if False: # first time true, else repo exists make false\n",
    "    create_repo(repo_name, token=my_token)\n",
    "\n",
    "# List all directories inside the model path\n",
    "folders = [f for f in os.listdir(model_path) if os.path.isdir(os.path.join(model_path, f))]\n",
    "skip_check = False\n",
    "\n",
    "# Print the list of folders\n",
    "if folders and not skip_check:\n",
    "    print(\"Folders inside 'model/':\")\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "else:\n",
    "    # Initialize the API\n",
    "    api = HfApi()\n",
    "\n",
    "    # Upload the model\n",
    "    api.upload_folder(\n",
    "        folder_path=model_path,\n",
    "        repo_id=repo_name,\n",
    "        commit_message=\"Second training\",\n",
    "        token=my_token\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
