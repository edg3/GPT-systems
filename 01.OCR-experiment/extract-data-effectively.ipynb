{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4f8111-d70b-4298-a7ad-1421ba495040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pillow==10.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44845b72-b16a-4ab9-91b8-cb9f6d752b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c055681a-81cf-4a0c-84ba-5b979889a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision==0.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee357f8-29af-4bbe-bd7d-bdcb554941e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa0af81-17ea-4ce9-be8e-26169421783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece==0.1.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e384bc58-9869-4518-a866-dfd51801ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4bbeb75-d4be-4486-b54e-0201d39d7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bitsandbytes==0.43.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5fc9e29-29fb-4623-8403-a54f3fbf4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f139e4df-aeed-417b-9a36-5b5b40a1715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d59ea1cb3fb4e388c71972bb9b6a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniCPMV(\n",
       "  (llm): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (vpm): Idefics2VisionTransformer(\n",
       "    (embeddings): Idefics2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      (position_embedding): Embedding(4900, 1152)\n",
       "    )\n",
       "    (encoder): Idefics2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x Idefics2EncoderLayer(\n",
       "          (self_attn): Idefics2VisionAttention(\n",
       "            (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Idefics2VisionMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (resampler): Resampler(\n",
       "    (kv_proj): Linear(in_features=1152, out_features=4096, bias=False)\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (ln_q): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_kv): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_post): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model, create function\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import time\n",
    "import os\n",
    "\n",
    "model = AutoModel.from_pretrained('MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73ec1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(input_image_path, output_image_path, width=None):\n",
    "    original_image = Image.open(input_image_path).convert('RGB')\n",
    "    w, h = original_image.size\n",
    "    if width is not None:\n",
    "        # calculate the height using the same aspect ratio\n",
    "        height = int(h * width / w)\n",
    "        resized_img = original_image.resize((width, height))\n",
    "    else:\n",
    "        resized_img = original_image\n",
    "    resized_img.save(output_image_path)\n",
    "\n",
    "def ProcessImage(img, question):\n",
    "    start_time = time.time()\n",
    "    image = Image.open(img).convert('RGB')\n",
    "    if not os.path.isfile('out-' + img):\n",
    "        scale_image(img, 'out-' + img, width=800)\n",
    "    image = Image.open('out-' + img).convert('RGB')\n",
    "    msgs = [{'role': 'user', 'content': question}]\n",
    "    res = model.chat(\n",
    "        image=image,\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        sampling=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(res)\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    minutes, seconds = divmod(time_diff,60)\n",
    "    print(\"[{0} min, {1} sec]\".format(int(minutes), int(seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adff0f3-b4d8-4bc7-8842-0921dfebf5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"store_name\": \"KFC South Africa Copper Moon Trading As KFC Rosebank\",\n",
      "  \"date_time\": \"Nov'01\" 18:12:12,\n",
      "  \"total\": 49.90,\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"name\": \"1 Chicken LunchBox\",\n",
      "      \"quantity\": 1,\n",
      "      \"price\": 49.90\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"1 Small Lbox M1 S/SF\",\n",
      "      \"quantity\": 1,\n",
      "      \"price\": 0.00\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[14 min, 44 sec]\n"
     ]
    }
   ],
   "source": [
    "ProcessImage(\n",
    "  \"000sample.jpg\",\n",
    "  \"convert this image to a json structure answer, summarise all content \" +\n",
    "  \"that is either a store_name, date_time, total, and items with name, \" +\n",
    "  \"quantity, and price, ignoring anything with names 'cash' or 'credit' \" +\n",
    "  \"in the image it's a till slip with the store name which doesn't need \" +\n",
    "  \"ownership details, the payment amount, and a json array with each \" +\n",
    "  \"line item purchased with focus on name of item, the quantity received\" +\n",
    "  \", and the price of the listed item, while ignoring items that are \" +\n",
    "  \"just additional business info as well, or have price '0.00'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd624291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main points highlighted in your resume as a full-stack developer include:\n",
      "\n",
      "1. Proficiency in various technologies such as MSSQL, MySQL, PHP, Python, and JavaScript.\n",
      "2. Experience with software development, including data analysis, management, and implementation.\n",
      "3. Familiarity with version control systems like Git and web development frameworks like Bootstrap.\n",
      "4. Knowledge of machine learning, artificial intelligence, and optical character recognition (OCR) concepts.\n",
      "5. A history of working on personal projects and contributions to open-source platforms.\n",
      "\n",
      "To improve your full-stack developer mindset and broaden your knowledge in AI, ML, and OCR, you can consider working on projects that integrate these technologies. For instance, you could create a web application that uses OCR to recognize and extract information from scanned documents, or build an AI-powered recommendation system for products. Additionally, you could explore ML-based projects such as sentiment analysis on customer reviews or image classification using deep learning algorithms. By documenting these projects on your GitHub and writing blog posts about them, you can showcase your skills and interests effectively on your LinkedIn profile. This approach not only demonstrates your technical abilities but also your ability to apply them in real-world scenarios, which is highly valued by potential employers and collaborators.\n",
      "[5 min, 5 sec]\n"
     ]
    }
   ],
   "source": [
    "ProcessImage(\n",
    "    \"cv-ernestl.png\",\n",
    "    \"as a professional full stack developer, what are the main best points \" +\n",
    "    \"in my resume, and then what full stack project(s) can I do to improve \" +\n",
    "    \"my full-stack developer mindset, as well as broaden my knowledge in AI\" +\n",
    "    \", ML, and OCR, since I want to document all my knowledge in sample \" +\n",
    "    \"projects in all of my skills on my github. This intends to broaden my \" +\n",
    "    \"LinkedIn profile in the broadest way, and I can make each project into\" +\n",
    "    \" a blog post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f18eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
